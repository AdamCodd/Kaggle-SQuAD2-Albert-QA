{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install --upgrade pytorch_lightning\n","!pip install --upgrade transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","import json\n","import os\n","import pandas as pd\n","import pytorch_lightning as pl\n","from pandas import DataFrame\n","from transformers import AlbertTokenizerFast, AlbertForQuestionAnswering\n","from transformers import get_linear_schedule_with_warmup\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.metrics import f1_score\n","from pytorch_lightning import seed_everything\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","import torch.nn.functional as F\n","import random\n","import numpy as np\n","\n","# Generate a random seed\n","#random_seed = random.randint(0, 10000)\n","random_seed = 1270\n","\n","# Set the a fixed seed for reproducibility\n","seed_everything(random_seed)\n","\n","# Print the random seed for reference\n","print(f\"Random seed used: {random_seed}\")\n","\n","# Constants\n","print(f\"CPU cores {os.cpu_count()}\")\n","MAX_LENGTH = 384  # SQuAD requires longer sequences\n","TRAIN_BATCH_SIZE = 12  # Reduced for SQuAD\n","VAL_BATCH_SIZE = 18  # Reduced for SQuAD\n","NUM_EPOCHS = 3\n","LEARNING_RATE = 3e-5\n","\n","print(f\"Learning Rate: {LEARNING_RATE} / Epochs: {NUM_EPOCHS}\")\n","\n","def prepare_qa_generator(path):\n","    with open(path, 'r') as f:\n","        squad_dict = json.load(f)\n","        \n","    for group in squad_dict['data']:\n","        for passage in group['paragraphs']:\n","            context = passage['context']\n","            for qa in passage['qas']:\n","                question = qa['question']\n","                qid = qa['id']\n","                for answer in qa['answers']:\n","                    answer_start = answer['answer_start']\n","                    text = answer['text']\n","                    answer_end = answer_start + len(text)\n","                    is_impossible = qa['is_impossible']    \n","                    yield {\n","                        'id': qid,\n","                        'context': context,\n","                        'question': question,\n","                        'answer_start': answer_start,\n","                        'answer_end': answer_end,\n","                        'text': text,\n","                        'is_impossible': is_impossible,\n","                    }\n","\n","def prepare_qa(path):\n","    return pd.DataFrame(prepare_qa_generator(path))\n","\n","def modify_answer_context(df):\n","    for idx, row in df.iterrows():\n","        ctx = row['context']\n","        result_text = row['text']\n","        start_idx = row['answer_start']\n","        end_idx = start_idx + len(result_text)\n","        \n","        if ctx[start_idx:end_idx] == result_text:\n","            df.at[idx, 'answer_end'] = end_idx\n","        elif ctx[start_idx-1:end_idx-1] == result_text:\n","            df.at[idx, 'answer_start'] = start_idx - 1\n","            df.at[idx, 'answer_end'] = end_idx - 1\n","        elif ctx[start_idx-2:end_idx-2] == result_text:\n","            df.at[idx, 'answer_start'] = start_idx - 2\n","            df.at[idx, 'answer_end'] = end_idx - 2\n","        else:\n","            raise ValueError(\"Answer indices do not match with the context text.\")\n","\n","train_df = prepare_qa('/kaggle/input/squad-2/train-v2.0.json')\n","val_df = prepare_qa('/kaggle/input/squad-2/dev-v2.0.json')            \n","\n","# In-place modification of DataFrame\n","modify_answer_context(train_df)\n","modify_answer_context(val_df)\n","\n","# Custom Dataset class for DataLoader\n","print(\"Preparing custom dataset...\")\n","# Custom Dataset class for DataLoader\n","class SquadDataset(Dataset):\n","    def __init__(self, df):\n","        self.df = df\n","        # Initialize the tokenizer\n","        print(\"Initializing tokenizer...\")\n","        self.tokenizer = AlbertTokenizerFast.from_pretrained('albert-base-v2')\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        is_impossible = row['is_impossible']\n","        encodings = self.tokenizer(row['context'], row['question'], truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=\"pt\")\n","        start_positions = encodings.char_to_token(0, row['answer_start'])\n","        end_positions = encodings.char_to_token(0, row['answer_end'] - 1)\n","\n","        if start_positions is None:\n","            start_positions = self.tokenizer.model_max_length\n","        if end_positions is None:\n","            end_positions = self.tokenizer.model_max_length\n","\n","        encodings.update({'start_positions': start_positions, 'end_positions': end_positions, 'id': row['id']})\n","        encodings.update({'is_impossible': is_impossible})\n","        return {key: val.clone().detach() if torch.is_tensor(val) else torch.tensor(bool(val)) if isinstance(val, np.bool_) else torch.tensor(val) if not isinstance(val, str) else val for key, val in encodings.items()}\n","   \n","    \n","# Create datasets\n","print(\"Creating datasets...\")\n","train_dataset = SquadDataset(train_df)\n","val_dataset = SquadDataset(val_df)\n","\n","#Config\n","albert_config = {\n","  \"architectures\": [\"AlbertForQuestionAnswering\"],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"output_past\": True,\n","  \"pad_token_id\": 0,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","# Define Model\n","print(\"Defining the model...\")\n","class SquadModel(pl.LightningModule):\n","    def __init__(self, lr):\n","        super().__init__()\n","        from transformers import AlbertConfig\n","        albert_configuration = AlbertConfig(**albert_config)\n","        self.model = AlbertForQuestionAnswering.from_pretrained('albert-base-v2', config=albert_configuration)\n","        self.loss = nn.CrossEntropyLoss()\n","        self.lr = lr  # set learning rate\n","        self.train_losses = []\n","        self.best_threshold = 0.0\n","        \n","        # Initialize variables for validation metrics\n","        self.val_exact_match_accumulated = 0\n","        self.val_f1_accumulated = 0\n","        self.val_count = 0\n","        self.all_no_answer_scores = []\n","        self.actual_no_answer = []\n","           \n","    def forward(self, inputs):\n","        inputs_without_id = {k: v.squeeze(1) if len(v.shape) == 3 else v for k, v in inputs.items() if k not in ['id', 'is_impossible']}\n","        outputs = self.model(**inputs_without_id)\n","        return outputs.loss, outputs.start_logits, outputs.end_logits\n","\n","    def training_step(self, batch, batch_idx):\n","        loss, start_logits, end_logits = self.forward(batch)\n","        self.train_losses.append(loss)\n","\n","        start_preds = torch.argmax(start_logits, dim=1)\n","        end_preds = torch.argmax(end_logits, dim=1)\n","\n","        start_positions = batch['start_positions']\n","        end_positions = batch['end_positions']\n","        is_impossible = batch['is_impossible']\n","\n","        # Exact Match\n","        actual_no_answer = is_impossible\n","        predicted_no_answer = (start_preds == MAX_LENGTH) & (end_preds == MAX_LENGTH)\n","\n","        exact_match = ((start_preds == start_positions) & (end_preds == end_positions)) | (predicted_no_answer & actual_no_answer)\n","        exact_match = exact_match.float().sum()\n","\n","        # F1 Score calculation\n","        f1s = []\n","        for i in range(len(start_preds)):\n","            pred_range = list(range(start_preds[i], end_preds[i] + 1))\n","            true_range = list(range(start_positions[i], end_positions[i] + 1))\n","            common = set(pred_range).intersection(true_range)\n","            f1 = 2 * len(common) / (len(pred_range) + len(true_range))\n","            f1s.append(f1)\n","        f1 = torch.tensor(f1s).mean()\n","\n","        # Log metrics\n","        self.log('train_loss', loss, prog_bar=True, sync_dist=True, batch_size=TRAIN_BATCH_SIZE)\n","        self.log('train_exact_match', exact_match / len(start_preds), prog_bar=True, sync_dist=True, batch_size=TRAIN_BATCH_SIZE)\n","        self.log('train_f1', f1, prog_bar=True, sync_dist=True, batch_size=TRAIN_BATCH_SIZE)\n","\n","        return loss\n","\n","    def on_train_epoch_end(self):\n","        avg_loss = torch.stack(self.train_losses).mean()  # Compute average loss\n","        self.log('avg_train_loss', avg_loss, sync_dist=True, batch_size=TRAIN_BATCH_SIZE)\n","        self.train_losses = []  # Clear the list for the next epoch\n","\n","    def configure_optimizers(self):\n","        optimizer = AdamW(self.parameters(), lr=self.lr)\n","        steps_per_epoch = len(train_dataset) // TRAIN_BATCH_SIZE\n","        total_steps = steps_per_epoch * NUM_EPOCHS\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=150, num_training_steps=total_steps)\n","        return [optimizer], [{'scheduler': scheduler, 'interval': 'step'}]\n","    \n","    def validation_step(self, batch, batch_idx):\n","        if not batch:\n","            return None  # Skip this batch if it's empty\n","        batch_inputs = {k: v for k, v in batch.items() if k != 'id'}\n","        loss, start_logits, end_logits = self.forward(batch_inputs)\n","\n","        start_preds = torch.argmax(start_logits, dim=1)\n","        end_preds = torch.argmax(end_logits, dim=1)\n","\n","        start_positions = batch['start_positions']\n","        end_positions = batch['end_positions']\n","        is_impossible = batch['is_impossible']\n","\n","        # Exact Match\n","        actual_no_answer = is_impossible\n","        predicted_no_answer = (start_preds == MAX_LENGTH) & (end_preds == MAX_LENGTH)\n","\n","        exact_match = ((start_preds == start_positions) & (end_preds == end_positions)) | (predicted_no_answer & actual_no_answer)\n","        exact_match = exact_match.float().sum()\n","\n","        # Compute \"no answer\" score\n","        no_answer_score_start = F.softmax(start_logits, dim=-1)[:, 0]\n","        no_answer_score_end = F.softmax(end_logits, dim=-1)[:, 0]\n","        # Store the no answer score for tuning the threshold later\n","        no_answer_score = torch.min(no_answer_score_start, no_answer_score_end)\n","\n","        # Append 'no answer' scores and labels for each batch\n","        self.all_no_answer_scores.append(no_answer_score)\n","        self.actual_no_answer.append(is_impossible)\n","        \n","        # Count 'no answer' only if it reaches the threshold\n","        threshold_value = self.best_threshold\n","        predicted_no_answer = no_answer_score > threshold_value\n","\n","        # F1 Score\n","        f1s = []\n","        for i in range(len(start_preds)):\n","            pred_range = list(range(start_preds[i], end_preds[i] + 1))\n","            true_range = list(range(start_positions[i], end_positions[i] + 1))\n","            common = set(pred_range).intersection(true_range)\n","            f1 = 2 * len(common) / (len(pred_range) + len(true_range))\n","            f1s.append(f1)\n","        f1 = torch.tensor(f1s).mean()\n","\n","        # Log metrics\n","        self.log('val_loss', loss, prog_bar=True, sync_dist=True, batch_size=VAL_BATCH_SIZE)\n","        self.log('val_exact_match', exact_match / len(start_preds), prog_bar=True, sync_dist=True, batch_size=VAL_BATCH_SIZE)\n","        self.log('val_f1', f1, prog_bar=True, sync_dist=True, batch_size=VAL_BATCH_SIZE)\n","\n","        # Accumulate the metrics\n","        self.val_exact_match_accumulated += exact_match.item()\n","        self.val_f1_accumulated += f1.item() * len(start_preds)\n","        self.val_count += len(start_preds)\n","\n","        return {'val_loss': loss, 'val_exact_match': exact_match, 'val_f1': f1}\n","\n","    def on_validation_epoch_end(self):\n","        # Clear variables for the next epoch\n","        self.val_exact_match_accumulated = 0\n","        self.val_f1_accumulated = 0\n","        self.val_count = 0\n","    \n","# Initialize the model\n","print(\"Initializing model...\")\n","train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=0)\n","val_dataloader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, num_workers=0)\n","\n","model = SquadModel(lr=LEARNING_RATE)\n","\n","# Create a ModelCheckpoint callback\n","checkpoint_callback = ModelCheckpoint(\n","    dirpath=\"/kaggle/working/checkpoints\",  # directory where checkpoints will be saved\n","    filename=\"squad-epoch{epoch:02d}\",  # checkpoint file name\n","    save_top_k=1,  # keep only the best checkpoint\n","    verbose=True,  # print saving info\n","    save_last=True,  # save the last epoch's checkpoint\n","    monitor=\"val_loss\",  # metric to monitor (could be validation loss, F1 score, etc.)\n","    mode=\"min\"  # save the model with the minimum validation loss\n",")\n","\n","# Initialize Trainer\n","print(\"Initializing trainer...\")\n","trainer = pl.Trainer(\n","    max_epochs=NUM_EPOCHS, # When loading from a checkpoint => It will train the model for NUM_EPOCHS more epochs\n","    accelerator=\"auto\",\n","    logger=pl.loggers.TensorBoardLogger('/kaggle/working/logs/', name='squad', version=0),\n","    callbacks=[checkpoint_callback],\n",")\n","print(\"End. Waiting for the next step.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Run learning rate finder (Doesn't work on multi-GPUs)\n","print(\"Running learning rate finder\")\n","tuner = pl.tuner.Tuner(trainer)\n","lr_finder = tuner.lr_find(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, min_lr=1e-6, max_lr=1.0, early_stop_threshold=None)\n","\n","# Print the results\n","print(\"LR Finder Results:\")\n","lr_finder.results\n","\n","# Plot the results\n","fig = lr_finder.plot(suggest=True)\n","if fig is not None:\n","    fig.show()\n","else:\n","    print(\"No figure to show.\")\n","\n","# Get the suggested learning rate\n","new_lr = lr_finder.suggestion()\n","if new_lr is not None:\n","    print(f\"Suggested new LR: {new_lr}\")\n","    #print(\"Setting a new instance with that LR\")\n","    #model = SquadModel(lr=new_lr)\n","else:\n","    print(\"No new LR suggested.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Wrap the training phase with try-except for error handling\n","try:\n","    # Train the model\n","    print(\"Training the model...\")\n","    trainer.fit(model, train_dataloader, val_dataloader)\n","except RuntimeError as e:\n","    if \"out of memory\" in str(e):\n","        print(\"ERROR: Out of memory\")\n","    else:\n","        print(\"An unexpected error occurred during training.\")\n","        print(str(e))\n","except Exception as e:\n","    print(\"An unexpected error occurred during training.\")\n","    print(str(e))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Save the model\n","print(\"Saving the model...\")\n","model.model.save_pretrained(\"/kaggle/working/\")\n","\n","# Save the tokenizer\n","tokenizer_save_path = \"/kaggle/working/\"\n","tokenizer.save_pretrained(tokenizer_save_path)\n","\n","# Save training args\n","training_args_dict = {\n","    \"_n_gpu\": 1,\n","    \"adafactor\": False,\n","    \"adam_beta1\": 0.9,\n","    \"adam_beta2\": 0.999,\n","    \"adam_epsilon\": 1e-08,\n","    \"auto_find_batch_size\": False,\n","    \"bf16\": False,\n","    \"bf16_full_eval\": False,\n","    \"data_seed\": None,\n","    \"dataloader_drop_last\": False,\n","    \"dataloader_num_workers\": 0,\n","    \"dataloader_pin_memory\": True,\n","    \"ddp_backend\": None,\n","    \"ddp_broadcast_buffers\": None,\n","    \"ddp_bucket_cap_mb\": None,\n","    \"ddp_find_unused_parameters\": None,\n","    \"ddp_timeout\": 1800,\n","    \"debug\": [],\n","    \"deepspeed\": None,\n","    \"disable_tqdm\": False,\n","    \"dispatch_batches\": None,\n","    \"do_eval\": True,\n","    \"do_predict\": False,\n","    \"do_train\": True,\n","    \"eval_accumulation_steps\": None,\n","    \"eval_delay\": 0,\n","    \"eval_steps\": None,\n","    \"evaluation_strategy\": \"epoch\",\n","    \"fp16\": False,\n","    \"fp16_backend\": \"auto\",\n","    \"fp16_full_eval\": False,\n","    \"fp16_opt_level\": \"O1\",\n","    \"fsdp\": \"\",\n","    \"fsdp_config\": None,\n","    \"fsdp_min_num_params\": 0,\n","    \"fsdp_transformer_layer_cls_to_wrap\": None,\n","    \"full_determinism\": False,\n","    \"gradient_accumulation_steps\": 1,\n","    \"gradient_checkpoisnting\": False,\n","    \"greater_is_better\": True,\n","    \"group_by_length\": False,\n","    \"half_precision_backend\": \"auto\",\n","    \"hub_always_push\": False,\n","    \"hub_model_id\": \"distilbert-base-uncased-squad2\",\n","    \"hub_private_repo\": False,\n","    \"hub_strategy\": \"every_save\",\n","    \"hub_token\": \"<HUB_TOKEN>\",\n","    \"ignore_data_skip\": False,\n","    \"include_inputs_for_metrics\": False,\n","    \"jit_mode_eval\": False,\n","    \"label_names\": None,\n","    \"label_smoothing_factor\": 0.0,\n","    \"learning_rate\": 3e-05,\n","    \"length_column_name\": \"length\",\n","    \"load_best_model_at_end\": True,\n","    \"local_rank\": -1,\n","    \"log_level\": -1,\n","    \"log_level_replica\": -1,\n","    \"log_on_each_node\": True,\n","    \"logging_dir\": \"/opt/ml/output/data/logs\",\n","    \"logging_first_step\": False,\n","    \"logging_nan_inf_filter\": True,\n","    \"logging_steps\": 500,\n","    \"logging_strategy\": \"steps\",\n","    \"lr_scheduler_type\": \"linear\",\n","    \"max_grad_norm\": 1.0,\n","    \"max_steps\": -1,\n","    \"metric_for_best_model\": \"f1_score\",\n","    \"mp_parameters\": \"\",\n","    \"no_cuda\": False,\n","    \"num_train_epochs\": 3,\n","    \"optim\": \"adamw_torch\",\n","    \"optim_args\": None,\n","    \"output_dir\": \"/opt/ml/model\",\n","    \"overwrite_output_dir\": False,\n","    \"past_index\": -1,\n","    \"per_device_eval_batch_size\": 32,\n","    \"per_device_train_batch_size\": 24,\n","    \"prediction_loss_only\": False,\n","    \"push_to_hub\": False,\n","    \"push_to_hub_model_id\": None,\n","    \"push_to_hub_organization\": None,\n","    \"push_to_hub_token\": \"<PUSH_TO_HUB_TOKEN>\",\n","    \"ray_scope\": \"last\",\n","    \"remove_unused_columns\": True,\n","    \"report_to\": [\"tensorboard\"],\n","    \"resume_from_checkpoint\": None,\n","    \"run_name\": \"/opt/ml/model\",\n","    \"save_on_each_node\": False,\n","    \"save_safetensors\": False,\n","    \"save_steps\": 500,\n","    \"save_strategy\": \"epoch\",\n","    \"save_total_limit\": 2,\n","    \"seed\": 1270,\n","    \"sharded_ddp\": [],\n","    \"skip_memory_metrics\": True,\n","    \"tf32\": None,\n","    \"torch_compile\": False,\n","    \"torch_compile_backend\": None,\n","    \"torch_compile_mode\": None,\n","    \"torchdynamo\": None,\n","    \"tpu_metrics_debug\": False,\n","    \"tpu_num_cores\": None,\n","    \"use_cpu\": False,\n","    \"use_ipex\": False,\n","    \"use_legacy_prediction_loop\": False,\n","    \"use_mps_device\": False,\n","    \"warmup_ratio\": 0.0,\n","    \"warmup_steps\": 150,\n","    \"weight_decay\": 0.01,\n","}\n","torch.save(training_args_dict, \"/kaggle/working/training_args.bin\")\n","print(\"Model and config saved\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Test the model\n","from transformers import QuestionAnsweringPipeline\n","# Initialize the QA pipeline\n","tokenizer = AlbertTokenizerFast.from_pretrained('albert-base-v2')\n","qa_pipeline = QuestionAnsweringPipeline(model=model.model, tokenizer=tokenizer)  # Note the `model.model`\n","\n","# Ask a question\n","result = qa_pipeline(\n","    question=\"Which name is also used to describe the Amazon rainforest in English?\",\n","    context='''The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'''\n","    handle_impossible_answer=True  # important!\n",")\n","\n","print(result)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
