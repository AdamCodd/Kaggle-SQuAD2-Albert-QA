{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade pytorch_lightning\n!pip install --upgrade transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nimport json\nimport os\nimport pandas as pd\nimport pytorch_lightning as pl\nfrom pandas import DataFrame\nfrom transformers import AlbertTokenizerFast, AlbertForQuestionAnswering\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import GridSearchCV\nfrom pytorch_lightning import seed_everything\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport torch.nn.functional as F\nimport random\nimport numpy as np\n\n# Generate a random seed\n#random_seed = random.randint(0, 10000)\nrandom_seed = 1270\n\n# Set the a fixed seed for reproducibility\nseed_everything(random_seed)\n\n# Print the random seed for reference\nprint(f\"Random seed used: {random_seed}\")\n\n# Constants\nprint(f\"CPU cores {os.cpu_count()}\")\nMAX_LENGTH = 384  # SQuAD requires longer sequences\nTRAIN_BATCH_SIZE = 12  # Reduced for SQuAD\nVAL_BATCH_SIZE = 18  # Reduced for SQuAD\nNUM_EPOCHS = 3\nLEARNING_RATE = 3e-5\n\nprint(f\"Learning Rate: {LEARNING_RATE} / Epochs: {NUM_EPOCHS}\")\n\ndef prepare_qa_generator(path):\n    with open(path, 'r') as f:\n        squad_dict = json.load(f)\n        \n    for group in squad_dict['data']:\n        for passage in group['paragraphs']:\n            context = passage['context']\n            for qa in passage['qas']:\n                question = qa['question']\n                qid = qa['id']\n                for answer in qa['answers']:\n                    answer_start = answer['answer_start']\n                    text = answer['text']\n                    answer_end = answer_start + len(text)\n                    is_impossible = qa['is_impossible']    \n                    yield {\n                        'id': qid,\n                        'context': context,\n                        'question': question,\n                        'answer_start': answer_start,\n                        'answer_end': answer_end,\n                        'text': text,\n                        'is_impossible': is_impossible,\n                    }\n\ndef prepare_qa(path):\n    return pd.DataFrame(prepare_qa_generator(path))\n\ndef modify_answer_context(df):\n    for idx, row in df.iterrows():\n        ctx = row['context']\n        result_text = row['text']\n        start_idx = row['answer_start']\n        end_idx = start_idx + len(result_text)\n        \n        if ctx[start_idx:end_idx] == result_text:\n            df.at[idx, 'answer_end'] = end_idx\n        elif ctx[start_idx-1:end_idx-1] == result_text:\n            df.at[idx, 'answer_start'] = start_idx - 1\n            df.at[idx, 'answer_end'] = end_idx - 1\n        elif ctx[start_idx-2:end_idx-2] == result_text:\n            df.at[idx, 'answer_start'] = start_idx - 2\n            df.at[idx, 'answer_end'] = end_idx - 2\n        else:\n            raise ValueError(\"Answer indices do not match with the context text.\")\n\ntrain_df = prepare_qa('/kaggle/input/squad-2/train-v2.0.json')\nval_df = prepare_qa('/kaggle/input/squad-2/dev-v2.0.json')            \n\n# In-place modification of DataFrame\nmodify_answer_context(train_df)\nmodify_answer_context(val_df)\n\n# Custom Dataset class for DataLoader\nprint(\"Preparing custom dataset...\")\n# Custom Dataset class for DataLoader\nclass SquadDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n        # Initialize the tokenizer\n        print(\"Initializing tokenizer...\")\n        self.tokenizer = AlbertTokenizerFast.from_pretrained('albert-base-v2')\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        is_impossible = row['is_impossible']\n        encodings = self.tokenizer(row['context'], row['question'], truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=\"pt\")\n        start_positions = encodings.char_to_token(0, row['answer_start'])\n        end_positions = encodings.char_to_token(0, row['answer_end'] - 1)\n\n        if start_positions is None:\n            start_positions = self.tokenizer.model_max_length\n        if end_positions is None:\n            end_positions = self.tokenizer.model_max_length\n\n        encodings.update({'start_positions': start_positions, 'end_positions': end_positions, 'id': row['id']})\n        encodings.update({'is_impossible': is_impossible})\n        return {key: val.clone().detach() if torch.is_tensor(val) else torch.tensor(bool(val)) if isinstance(val, np.bool_) else torch.tensor(val) if not isinstance(val, str) else val for key, val in encodings.items()}\n   \n    \n# Create datasets\nprint(\"Creating datasets...\")\ntrain_dataset = SquadDataset(train_df)\nval_dataset = SquadDataset(val_df)\n\n#Config\nalbert_config = {\n  \"architectures\": [\"AlbertForQuestionAnswering\"],\n  \"attention_probs_dropout_prob\": 0,\n  \"bos_token_id\": 2,\n  \"classifier_dropout_prob\": 0.1,\n  \"down_scale_factor\": 1,\n  \"embedding_size\": 128,\n  \"eos_token_id\": 3,\n  \"gap_size\": 0,\n  \"hidden_act\": \"gelu_new\",\n  \"hidden_dropout_prob\": 0,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"inner_group_num\": 1,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"albert\",\n  \"net_structure_type\": 0,\n  \"num_attention_heads\": 12,\n  \"num_hidden_groups\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_memory_blocks\": 0,\n  \"output_past\": True,\n  \"pad_token_id\": 0,\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30000\n}\n\n# Define Model\nprint(\"Defining the model...\")\nclass SquadModel(pl.LightningModule):\n    def __init__(self, lr):\n        super().__init__()\n        from transformers import AlbertConfig\n        albert_configuration = AlbertConfig(**albert_config)\n        self.model = AlbertForQuestionAnswering.from_pretrained('albert-base-v2', config=albert_configuration)\n        self.loss = nn.CrossEntropyLoss()\n        self.lr = lr  # set learning rate\n        self.train_losses = []\n        self.best_threshold = 0.0\n        \n        # Initialize variables for validation metrics\n        self.val_exact_match_accumulated = 0\n        self.val_f1_accumulated = 0\n        self.val_count = 0\n        self.all_no_answer_scores = []\n        self.actual_no_answer = []\n           \n    def forward(self, inputs):\n        inputs_without_id = {k: v.squeeze(1) if len(v.shape) == 3 else v for k, v in inputs.items() if k not in ['id', 'is_impossible']}\n        outputs = self.model(**inputs_without_id)\n        return outputs.loss, outputs.start_logits, outputs.end_logits\n\n    def training_step(self, batch, batch_idx):\n        loss, start_logits, end_logits = self.forward(batch)\n        self.train_losses.append(loss)\n\n        start_preds = torch.argmax(start_logits, dim=1)\n        end_preds = torch.argmax(end_logits, dim=1)\n\n        start_positions = batch['start_positions']\n        end_positions = batch['end_positions']\n        is_impossible = batch['is_impossible']\n\n        # Exact Match\n        actual_no_answer = is_impossible\n        predicted_no_answer = (start_preds == MAX_LENGTH) & (end_preds == MAX_LENGTH)\n\n        exact_match = ((start_preds == start_positions) & (end_preds == end_positions)) | (predicted_no_answer & actual_no_answer)\n        exact_match = exact_match.float().sum()\n\n        # F1 Score calculation\n        f1s = []\n        for i in range(len(start_preds)):\n            pred_range = list(range(start_preds[i], end_preds[i] + 1))\n            true_range = list(range(start_positions[i], end_positions[i] + 1))\n            common = set(pred_range).intersection(true_range)\n            f1 = 2 * len(common) / (len(pred_range) + len(true_range))\n            f1s.append(f1)\n        f1 = torch.tensor(f1s).mean()\n\n        # Log metrics\n        self.log('train_loss', loss, prog_bar=True, sync_dist=True, batch_size=TRAIN_BATCH_SIZE)\n        self.log('train_exact_match', exact_match / len(start_preds), prog_bar=True, sync_dist=True, batch_size=TRAIN_BATCH_SIZE)\n        self.log('train_f1', f1, prog_bar=True, sync_dist=True, batch_size=TRAIN_BATCH_SIZE)\n\n        return loss\n\n    def on_train_epoch_end(self):\n        avg_loss = torch.stack(self.train_losses).mean()  # Compute average loss\n        self.log('avg_train_loss', avg_loss, sync_dist=True, batch_size=TRAIN_BATCH_SIZE)\n        self.train_losses = []  # Clear the list for the next epoch\n\n    def configure_optimizers(self):\n        optimizer = AdamW(self.parameters(), lr=self.lr)\n        steps_per_epoch = len(train_dataset) // TRAIN_BATCH_SIZE\n        total_steps = steps_per_epoch * NUM_EPOCHS\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=150, num_training_steps=total_steps)\n        return [optimizer], [{'scheduler': scheduler, 'interval': 'step'}]\n    \n    def validation_step(self, batch, batch_idx):\n        if not batch:\n            return None  # Skip this batch if it's empty\n        batch_inputs = {k: v for k, v in batch.items() if k != 'id'}\n        loss, start_logits, end_logits = self.forward(batch_inputs)\n\n        start_preds = torch.argmax(start_logits, dim=1)\n        end_preds = torch.argmax(end_logits, dim=1)\n\n        start_positions = batch['start_positions']\n        end_positions = batch['end_positions']\n        is_impossible = batch['is_impossible']\n\n        # Exact Match\n        actual_no_answer = is_impossible\n        predicted_no_answer = (start_preds == MAX_LENGTH) & (end_preds == MAX_LENGTH)\n\n        exact_match = ((start_preds == start_positions) & (end_preds == end_positions)) | (predicted_no_answer & actual_no_answer)\n        exact_match = exact_match.float().sum()\n\n        # Compute \"no answer\" score\n        no_answer_score_start = F.softmax(start_logits, dim=-1)[:, 0]\n        no_answer_score_end = F.softmax(end_logits, dim=-1)[:, 0]\n        # Store the no answer score for tuning the threshold later\n        no_answer_score = torch.min(no_answer_score_start, no_answer_score_end)\n\n        # Append 'no answer' scores and labels for each batch\n        self.all_no_answer_scores.append(no_answer_score)\n        self.actual_no_answer.append(is_impossible)\n        \n        # Count 'no answer' only if it reaches the threshold\n        threshold_value = self.best_threshold\n        predicted_no_answer = no_answer_score > threshold_value\n\n        # F1 Score\n        f1s = []\n        for i in range(len(start_preds)):\n            pred_range = list(range(start_preds[i], end_preds[i] + 1))\n            true_range = list(range(start_positions[i], end_positions[i] + 1))\n            common = set(pred_range).intersection(true_range)\n            f1 = 2 * len(common) / (len(pred_range) + len(true_range))\n            f1s.append(f1)\n        f1 = torch.tensor(f1s).mean()\n\n        # Log metrics\n        self.log('val_loss', loss, prog_bar=True, sync_dist=True, batch_size=VAL_BATCH_SIZE)\n        self.log('val_exact_match', exact_match / len(start_preds), prog_bar=True, sync_dist=True, batch_size=VAL_BATCH_SIZE)\n        self.log('val_f1', f1, prog_bar=True, sync_dist=True, batch_size=VAL_BATCH_SIZE)\n\n        # Accumulate the metrics\n        self.val_exact_match_accumulated += exact_match.item()\n        self.val_f1_accumulated += f1.item() * len(start_preds)\n        self.val_count += len(start_preds)\n\n        return {'val_loss': loss, 'val_exact_match': exact_match, 'val_f1': f1}\n\n    def on_validation_epoch_end(self):\n        # Compute the average of the accumulated metrics\n        avg_val_exact_match = self.val_exact_match_accumulated / self.val_count\n        avg_val_f1 = self.val_f1_accumulated / self.val_count\n\n        # Log the average metrics\n        self.log('avg_val_exact_match', avg_val_exact_match, prog_bar=True, sync_dist=True, batch_size=VAL_BATCH_SIZE)\n        self.log('avg_val_f1', avg_val_f1, prog_bar=True, sync_dist=True, batch_size=VAL_BATCH_SIZE)\n\n        # Clear variables for the next epoch\n        self.val_exact_match_accumulated = 0\n        self.val_f1_accumulated = 0\n        self.val_count = 0\n    \n# Initialize the model\nprint(\"Initializing model...\")\ntrain_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=0)\nval_dataloader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, num_workers=0)\n\nmodel = SquadModel(lr=LEARNING_RATE)\n\n# Create a ModelCheckpoint callback\ncheckpoint_callback = ModelCheckpoint(\n    dirpath=\"/kaggle/working/checkpoints\",  # directory where checkpoints will be saved\n    filename=\"squad-epoch{epoch:02d}\",  # checkpoint file name\n    save_top_k=1,  # keep only the best checkpoint\n    verbose=True,  # print saving info\n    save_last=True,  # save the last epoch's checkpoint\n    monitor=\"val_loss\",  # metric to monitor (could be validation loss, F1 score, etc.)\n    mode=\"min\"  # save the model with the minimum validation loss\n)\n\n# Initialize Trainer\nprint(\"Initializing trainer...\")\ntrainer = pl.Trainer(\n    max_epochs=NUM_EPOCHS, # When loading from a checkpoint => It will train the model for NUM_EPOCHS more epochs\n    accelerator=\"auto\",\n    logger=pl.loggers.TensorBoardLogger('/kaggle/working/logs/', name='squad', version=0),\n    callbacks=[checkpoint_callback],\n)\nprint(\"End. Waiting for the next step.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run learning rate finder (Doesn't work on multi-GPUs)\nprint(\"Running learning rate finder\")\ntuner = pl.tuner.Tuner(trainer)\nlr_finder = tuner.lr_find(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, min_lr=1e-6, max_lr=1.0, early_stop_threshold=None)\n\n# Print the results\nprint(\"LR Finder Results:\")\nlr_finder.results\n\n# Plot the results\nfig = lr_finder.plot(suggest=True)\nif fig is not None:\n    fig.show()\nelse:\n    print(\"No figure to show.\")\n\n# Get the suggested learning rate\nnew_lr = lr_finder.suggestion()\nif new_lr is not None:\n    print(f\"Suggested new LR: {new_lr}\")\n    #print(\"Setting a new instance with that LR\")\n    #model = SquadModel(lr=new_lr)\nelse:\n    print(\"No new LR suggested.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Wrap the training phase with try-except for error handling\ntry:\n    # Train the model\n    print(\"Training the model...\")\n    trainer.fit(model, train_dataloader, val_dataloader)\nexcept RuntimeError as e:\n    if \"out of memory\" in str(e):\n        print(\"ERROR: Out of memory\")\n    else:\n        print(\"An unexpected error occurred during training.\")\n        print(str(e))\nexcept Exception as e:\n    print(\"An unexpected error occurred during training.\")\n    print(str(e))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model\nprint(\"Saving the model...\")\nmodel.model.save_pretrained(\"/kaggle/working/\")\n\n# Save the tokenizer\ntokenizer_save_path = \"/kaggle/working/\"\ntokenizer.save_pretrained(tokenizer_save_path)\n\n# Save training args\ntraining_args_dict = {\n    \"_n_gpu\": 1,\n    \"adafactor\": False,\n    \"adam_beta1\": 0.9,\n    \"adam_beta2\": 0.999,\n    \"adam_epsilon\": 1e-08,\n    \"auto_find_batch_size\": False,\n    \"bf16\": False,\n    \"bf16_full_eval\": False,\n    \"data_seed\": None,\n    \"dataloader_drop_last\": False,\n    \"dataloader_num_workers\": 0,\n    \"dataloader_pin_memory\": True,\n    \"ddp_backend\": None,\n    \"ddp_broadcast_buffers\": None,\n    \"ddp_bucket_cap_mb\": None,\n    \"ddp_find_unused_parameters\": None,\n    \"ddp_timeout\": 1800,\n    \"debug\": [],\n    \"deepspeed\": None,\n    \"disable_tqdm\": False,\n    \"dispatch_batches\": None,\n    \"do_eval\": True,\n    \"do_predict\": False,\n    \"do_train\": True,\n    \"eval_accumulation_steps\": None,\n    \"eval_delay\": 0,\n    \"eval_steps\": None,\n    \"evaluation_strategy\": \"epoch\",\n    \"fp16\": False,\n    \"fp16_backend\": \"auto\",\n    \"fp16_full_eval\": False,\n    \"fp16_opt_level\": \"O1\",\n    \"fsdp\": \"\",\n    \"fsdp_config\": None,\n    \"fsdp_min_num_params\": 0,\n    \"fsdp_transformer_layer_cls_to_wrap\": None,\n    \"full_determinism\": False,\n    \"gradient_accumulation_steps\": 1,\n    \"gradient_checkpoisnting\": False,\n    \"greater_is_better\": True,\n    \"group_by_length\": False,\n    \"half_precision_backend\": \"auto\",\n    \"hub_always_push\": False,\n    \"hub_model_id\": \"distilbert-base-uncased-squad2\",\n    \"hub_private_repo\": False,\n    \"hub_strategy\": \"every_save\",\n    \"hub_token\": \"<HUB_TOKEN>\",\n    \"ignore_data_skip\": False,\n    \"include_inputs_for_metrics\": False,\n    \"jit_mode_eval\": False,\n    \"label_names\": None,\n    \"label_smoothing_factor\": 0.0,\n    \"learning_rate\": 3e-05,\n    \"length_column_name\": \"length\",\n    \"load_best_model_at_end\": True,\n    \"local_rank\": -1,\n    \"log_level\": -1,\n    \"log_level_replica\": -1,\n    \"log_on_each_node\": True,\n    \"logging_dir\": \"/opt/ml/output/data/logs\",\n    \"logging_first_step\": False,\n    \"logging_nan_inf_filter\": True,\n    \"logging_steps\": 500,\n    \"logging_strategy\": \"steps\",\n    \"lr_scheduler_type\": \"linear\",\n    \"max_grad_norm\": 1.0,\n    \"max_steps\": -1,\n    \"metric_for_best_model\": \"f1_score\",\n    \"mp_parameters\": \"\",\n    \"no_cuda\": False,\n    \"num_train_epochs\": 3,\n    \"optim\": \"adamw_torch\",\n    \"optim_args\": None,\n    \"output_dir\": \"/opt/ml/model\",\n    \"overwrite_output_dir\": False,\n    \"past_index\": -1,\n    \"per_device_eval_batch_size\": 32,\n    \"per_device_train_batch_size\": 24,\n    \"prediction_loss_only\": False,\n    \"push_to_hub\": False,\n    \"push_to_hub_model_id\": None,\n    \"push_to_hub_organization\": None,\n    \"push_to_hub_token\": \"<PUSH_TO_HUB_TOKEN>\",\n    \"ray_scope\": \"last\",\n    \"remove_unused_columns\": True,\n    \"report_to\": [\"tensorboard\"],\n    \"resume_from_checkpoint\": None,\n    \"run_name\": \"/opt/ml/model\",\n    \"save_on_each_node\": False,\n    \"save_safetensors\": False,\n    \"save_steps\": 500,\n    \"save_strategy\": \"epoch\",\n    \"save_total_limit\": 2,\n    \"seed\": 1270,\n    \"sharded_ddp\": [],\n    \"skip_memory_metrics\": True,\n    \"tf32\": None,\n    \"torch_compile\": False,\n    \"torch_compile_backend\": None,\n    \"torch_compile_mode\": None,\n    \"torchdynamo\": None,\n    \"tpu_metrics_debug\": False,\n    \"tpu_num_cores\": None,\n    \"use_cpu\": False,\n    \"use_ipex\": False,\n    \"use_legacy_prediction_loop\": False,\n    \"use_mps_device\": False,\n    \"warmup_ratio\": 0.0,\n    \"warmup_steps\": 150,\n    \"weight_decay\": 0.01,\n}\ntorch.save(training_args_dict, \"/kaggle/working/training_args.bin\")\nprint(\"Model and config saved\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the model\nfrom transformers import QuestionAnsweringPipeline\n# Initialize the QA pipeline\ntokenizer = AlbertTokenizerFast.from_pretrained('albert-base-v2')\nqa_pipeline = QuestionAnsweringPipeline(model=model.model, tokenizer=tokenizer)  # Note the `model.model`\n\n# Ask a question\nresult = qa_pipeline(\n    question=\"Which name is also used to describe the Amazon rainforest in English?\",\n    context='''The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'''\n    handle_impossible_answer=True  # important!\n)\n\nprint(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}